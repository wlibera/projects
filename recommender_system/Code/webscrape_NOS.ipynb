{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0cd7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Change these dates based on which year you are scraping for\n",
    "# insert dates\n",
    "dates = pd.date_range(start = \"2022-01-01\", end = \"2022-12-31\")\n",
    "year = '2022'\n",
    "\n",
    "# generate filenames\n",
    "metaArticles_temp_filename = 'metaArticles_temp_'+year+'.csv'\n",
    "metaArticles_filename = 'metaArticles_'+year+'.csv'\n",
    "\n",
    "articles_temp_filename = 'articles_temp_'+year+'.csv'\n",
    "articles_filename = 'articles_'+year+'.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae80a53d",
   "metadata": {
    "id": "ae80a53d"
   },
   "outputs": [],
   "source": [
    "# to get the URL\n",
    "import requests\n",
    "\n",
    "# to parse the HTMLDOM\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# patience is virtue\n",
    "import time\n",
    "\n",
    "# %pip install selenium\n",
    "# virtual browser\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.select import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e59f168",
   "metadata": {},
   "source": [
    "# Scrape article meta infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e34581cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_metaInfo(soup, df):\n",
    "    elements = soup.find_all(class_ = 'link-block')\n",
    "    for element in elements:\n",
    "\n",
    "        # get URL of element\n",
    "        url = element['href']\n",
    "        isArticle = url.startswith('/artikel') # check whether URL leads to an article (instead of e.g. a video)\n",
    "\n",
    "        if isArticle:\n",
    "            url = 'https://nos.nl' + url\n",
    "            # print(url)\n",
    "\n",
    "            # get title of element\n",
    "            title = element.find('div', {'class': 'list-time__title link-hover'}).text\n",
    "\n",
    "            newRow = pd.DataFrame({'url':[url], 'title':[title]})\n",
    "            df = pd.concat([df, newRow], ignore_index=True)\n",
    "\n",
    "        else:\n",
    "            next # skips this element if it refers to something besides an article\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c82fd3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def scrape_articlesMeta(daterange):\n",
    "    \"\"\"\n",
    "    Scrapes URL, title, and date from search results on NOS\n",
    "    \n",
    "    Parameters:\n",
    "        - daterange: range of dates created using\n",
    "          daterange = pd.date_range(start = \"2018-09-09\", end = datetime.today())\n",
    "    \n",
    "    Returns:\n",
    "        - dataframe that contains the columns outlet, url, title, and date\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialise driver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--incognito\")\n",
    "    driver = webdriver.Chrome(options = options) \n",
    "\n",
    "    # initialise dataframe\n",
    "    df = pd.DataFrame(data = {'url': [], 'title': []})\n",
    "\n",
    "    # make list of URLs\n",
    "    listURLs = [('https://nos.nl/nieuws/buitenland/archief/' + str(date.date())) for date in daterange]\n",
    "\n",
    "    # loop over urls in listURLs for scraping\n",
    "    for url in listURLs:\n",
    "        print('Scraping ' + url + ' ...')\n",
    "\n",
    "        # open page\n",
    "        driver.get(url)\n",
    "        driver.set_window_size(838, 900)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source)\n",
    "\n",
    "        df = scrape_metaInfo(soup, df) # this function should scrape all necessary data from the soup and save it to a dataframe\n",
    "        \n",
    "        # save as csv files\n",
    "        df.to_csv(metaArticles_temp_filename, index=False)\n",
    "        print('Total number of articles: ' + str(len(df)))\n",
    "\n",
    "        time.sleep(randint(1, 10)) # to not get blocked by the website\n",
    "\n",
    "\n",
    "    driver.close() # close tab\n",
    "\n",
    "    n_articles = len(df)\n",
    "    print(str(n_articles) + ' articles were scraped.')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77fff315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://nos.nl/nieuws/buitenland/archief/2019-01-01 ...\n",
      "Total number of articles: 16\n",
      "Scraping https://nos.nl/nieuws/buitenland/archief/2019-01-02 ...\n",
      "Total number of articles: 30\n",
      "Scraping https://nos.nl/nieuws/buitenland/archief/2019-01-03 ...\n",
      "Total number of articles: 48\n",
      "Scraping https://nos.nl/nieuws/buitenland/archief/2019-01-04 ...\n",
      "Total number of articles: 62\n",
      "Scraping https://nos.nl/nieuws/buitenland/archief/2019-01-05 ...\n",
      "Total number of articles: 79\n",
      "79 articles were scraped.\n"
     ]
    }
   ],
   "source": [
    "# datetime.today()\n",
    "metaArticles = scrape_articlesMeta(daterange = dates)\n",
    "\n",
    "# save as csv files\n",
    "metaArticles.to_csv(metaArticles_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71af5dd",
   "metadata": {},
   "source": [
    "# Scrape articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f22de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metaArticles = pd.read_csv('D:/OneDrive - Universiteit Utrecht/PER3_PersonalizationForPublicMedia/assignment2/General/Dataset/metaArticles_2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2aee4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_paragraphs(soup, df, url, title):\n",
    "\n",
    "    article = soup.find_all('p', class_ = 'sc-5a7f8528-0 ihDdJT')\n",
    "    article = ' '.join([paragraph.text for paragraph in article])\n",
    "\n",
    "    date = soup.find('time')['datetime'][:10]\n",
    "\n",
    "    tags = soup.find_all('p', class_ = 'sc-370380d-7 fmeDxO')\n",
    "    tags = [tag.text for tag in tags]\n",
    "    \n",
    "    try:\n",
    "        image = soup.find('img', class_ = 'sc-8e313b0a-1 dGrJDt')\n",
    "        image = image['src']\n",
    "    except:\n",
    "        image = ''    \n",
    "\n",
    "    newRow = pd.DataFrame({'url':[url], 'title': [title], 'article': [article], 'date': [date], 'tags': [tags], 'image': [image]})\n",
    "    df = pd.concat([df, newRow], ignore_index=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83ed507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def scrape_articles(metaArticles):\n",
    "    \"\"\"\n",
    "    Scrapes paragraphs from articles on NOS\n",
    "    \n",
    "    Parameters:\n",
    "        - metaArticles - dataframe that contains a column called 'url' and one called 'title' with all urls to be scraped\n",
    "    \n",
    "    Returns:\n",
    "        - dataframe that contains the columns url, paragraph and leadParagraph\n",
    "    \"\"\"\n",
    "    # initialise dataframe\n",
    "    df = pd.DataFrame(data = {'url': [], 'title': [], 'article': [], 'date': [], 'tags': [], 'image': []})\n",
    "\n",
    "    for row in tqdm(range(len(metaArticles.index))): # loop over row indices of metaArticles\n",
    "        url = metaArticles.iloc[row]['url']\n",
    "\n",
    "        # get data from the url\n",
    "        res = requests.get(url)\n",
    "\n",
    "        # parse to bs4\n",
    "        soup = BeautifulSoup(res.content)\n",
    "\n",
    "        df = scrape_paragraphs(soup, df, url, metaArticles.iloc[row]['title']) # this function should scrape all necessary data from the soup and save it to a dataframe\n",
    "        \n",
    "        # save as csv files\n",
    "        df.to_csv(articles_temp_filename, index=False, sep = '|')\n",
    "\n",
    "        time.sleep(randint(1, 20)) # to not get blocked by the website\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ff87fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0e314550604794a77fe36fc92c950f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3605 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "articles = scrape_articles(metaArticles[1377:])\n",
    "articles.to_csv(articles_filename, index=False, sep = '|')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c0014d7bdea8e8a3a940a8951e0e4dbb3d2b571e022e3cbf52dd9dc692d11f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
