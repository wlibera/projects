{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create_articlesEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\xenia\\\\OneDrive\\\\Desktop\\\\Personalisation in public media\\\\Assignment_2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load packages\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open cleaned data\n",
    "with open(\"C:/Users/xenia/OneDrive/Desktop/Personalisation in public media/Assignment_2/articles_cleaned.pkl\", \"rb\") as file:\n",
    "    articles = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec model \n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "#vector model\n",
    "def getWordVectors(tokenized_texts):\n",
    "    SIZE = 100 # dimensions of the embeddings\n",
    "    SG = 1 # whether to use skip-gram or CBOW (we use skip-gram)\n",
    "    WINDOW = 10 # the window size\n",
    "    N_WORKERS = 1 # number of workers to use\n",
    "    MIN_COUNT = 1\n",
    "\n",
    "    model = Word2Vec(vector_size=SIZE,\n",
    "                    sg=SG,\n",
    "                    window=WINDOW, \n",
    "                    min_count=MIN_COUNT,\n",
    "                    workers=N_WORKERS)\n",
    "\n",
    "    model.build_vocab(tokenized_texts)\n",
    "\n",
    "    model.train(tokenized_texts,\n",
    "            total_examples=model.corpus_count,\n",
    "            epochs=model.epochs) \n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# article vectors\n",
    "articles_vectors = getWordVectors(articles[\"cleaned_article\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find similar words function --> finds the top 10 similar words for each trigger \n",
    "def findSimilarWords(wordEmbeddings, wordDict):\n",
    "    similarWordsDict = {}\n",
    "    for word in wordDict:\n",
    "        # check if the word is present in the vocabulary\n",
    "        if word in wordEmbeddings.wv.key_to_index:\n",
    "            # add similar words for this word to the dictionary, including the word itself\n",
    "            similarWordsDict[word] = [word] + [w[0] for w in wordEmbeddings.wv.most_similar(word, topn=10)]\n",
    "    return similarWordsDict\n",
    "\n",
    "# Top N similar words for the articles\n",
    "# trigger words are the same used in the survey translated in Dutch\n",
    "# Some of these, we ended up not using because they appeared too little in the corpus to\n",
    "# Make good embeddings\n",
    "trigger_words = {\n",
    "    \"zelfmoord\": None,\n",
    "    \"ongelukken\": None,\n",
    "    \"fysieke_mishandeling\": None,\n",
    "    \"seksueel_misbruik\": None,\n",
    "    \"zelfbeschadiging\": None,\n",
    "    \"depressie\": None,\n",
    "    \"racisme\": None,\n",
    "    \"LGBTQ+-discriminatie\": None,\n",
    "    \"eetstoornissen\": None,\n",
    "    \"gehandicaptendiscriminatie\": None,\n",
    "    \"dierenmishandeling\": None\n",
    "}\n",
    "similarWords_articles = findSimilarWords(articles_vectors,trigger_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trigger concept vectors by averaging the word vectors for the trigger words and the top 10 similar words associated with each trigger concept.    \n",
    "\n",
    "def create_trigger_vecs(similarWords_articles,articles_vectors):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - similarWords_articles(dict): A dictionary where keys are trigger concepts and values are lists of words associated with each trigger created in previous code.\n",
    "    - article_vectors (KeyedVectors): Pre-trained word vectors from previous code.\n",
    "    \"\"\"\n",
    "    trigger_vecs = {}\n",
    "    for trigger, words in similarWords_articles.items():\n",
    "        vecs = [articles_vectors.wv.get_vector(word) for word in words if word in articles_vectors.wv.key_to_index]\n",
    "        trigger_vecs[trigger] = np.mean(vecs, axis=0)\n",
    "\n",
    "    return trigger_vecs # A dictionary where keys are trigger concepts and values are mean vectors for the associated words.\n",
    "    \n",
    "trigger_vecs = create_trigger_vecs(similarWords_articles,articles_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized code using chatgp\n",
    "\n",
    "def calculate_trigger_scores_df(articles, trigger_vecs):\n",
    "     # Create a new DataFrame with the same index as the input articles DataFrame and columns for each trigger word\n",
    "    trigger_scores_df = pd.DataFrame(index=articles.index, columns=trigger_vecs.keys())\n",
    "\n",
    "    # precompute article vectors\n",
    "    article_vecs = []\n",
    "\n",
    "     #  average vector representations of each article in a list\n",
    "    for article_idx in articles.index:\n",
    "        article_text = ' '.join(articles.loc[article_idx, 'cleaned_article'])\n",
    "        word_vecs = [articles_vectors.wv.get_vector(word) for word in article_text.split() if word in articles_vectors.wv.key_to_index]\n",
    "        article_vec = np.mean(word_vecs, axis=0)\n",
    "        article_vecs.append(article_vec)\n",
    "\n",
    "    \n",
    "    # average vector representation of each trigger and trigger words(top 10 most similar words to trigger)\n",
    "    for trigger in trigger_vecs:\n",
    "        similar_words = [word[0] for word in articles_vectors.wv.most_similar(positive=[trigger], topn=10)]\n",
    "        similar_vecs = [articles_vectors.wv.get_vector(word) for word in similar_words if word in articles_vectors.wv.key_to_index]\n",
    "        similar_vecs.append(trigger_vecs[trigger])\n",
    "        combined_vec = np.mean(similar_vecs, axis=0)\n",
    "        # the cosine similarity between the article vector and the combined vector(includes trigger word + top 10 most similar words) for the current trigger word\n",
    "        for i, article_idx in enumerate(articles.index):\n",
    "            article_vec = article_vecs[i]\n",
    "            similarity = np.dot(article_vec, combined_vec) / (np.linalg.norm(article_vec) * np.linalg.norm(combined_vec))\n",
    "            trigger_scores_df.at[article_idx, trigger] = similarity\n",
    "\n",
    "        \n",
    "    # the trigger_scores_df DataFrame as output with each trigger scores per trigger per article\n",
    "    return trigger_scores_df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guard_suicide</th>\n",
       "      <th>guard_accidents</th>\n",
       "      <th>guard_selfHarm</th>\n",
       "      <th>guard_depression</th>\n",
       "      <th>guard_racism</th>\n",
       "      <th>guard_eatingDisorders</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.68904</td>\n",
       "      <td>0.7962</td>\n",
       "      <td>0.791906</td>\n",
       "      <td>0.673404</td>\n",
       "      <td>0.623649</td>\n",
       "      <td>0.817572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.683451</td>\n",
       "      <td>0.825734</td>\n",
       "      <td>0.757744</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.538758</td>\n",
       "      <td>0.753499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.619136</td>\n",
       "      <td>0.794339</td>\n",
       "      <td>0.764686</td>\n",
       "      <td>0.650075</td>\n",
       "      <td>0.465704</td>\n",
       "      <td>0.784149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.654318</td>\n",
       "      <td>0.664056</td>\n",
       "      <td>0.791714</td>\n",
       "      <td>0.648363</td>\n",
       "      <td>0.640479</td>\n",
       "      <td>0.819716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.618715</td>\n",
       "      <td>0.746867</td>\n",
       "      <td>0.758392</td>\n",
       "      <td>0.577344</td>\n",
       "      <td>0.443122</td>\n",
       "      <td>0.75944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22596</th>\n",
       "      <td>0.621547</td>\n",
       "      <td>0.670748</td>\n",
       "      <td>0.774785</td>\n",
       "      <td>0.634767</td>\n",
       "      <td>0.519427</td>\n",
       "      <td>0.774898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22597</th>\n",
       "      <td>0.659439</td>\n",
       "      <td>0.904112</td>\n",
       "      <td>0.777663</td>\n",
       "      <td>0.685988</td>\n",
       "      <td>0.485242</td>\n",
       "      <td>0.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22598</th>\n",
       "      <td>0.743001</td>\n",
       "      <td>0.752128</td>\n",
       "      <td>0.81698</td>\n",
       "      <td>0.669132</td>\n",
       "      <td>0.658693</td>\n",
       "      <td>0.829151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22599</th>\n",
       "      <td>0.618995</td>\n",
       "      <td>0.703332</td>\n",
       "      <td>0.785767</td>\n",
       "      <td>0.622811</td>\n",
       "      <td>0.562866</td>\n",
       "      <td>0.796512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22600</th>\n",
       "      <td>0.643032</td>\n",
       "      <td>0.913813</td>\n",
       "      <td>0.759902</td>\n",
       "      <td>0.644197</td>\n",
       "      <td>0.465164</td>\n",
       "      <td>0.765236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22592 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      guard_suicide guard_accidents guard_selfHarm guard_depression  \\\n",
       "0           0.68904          0.7962       0.791906         0.673404   \n",
       "1          0.683451        0.825734       0.757744            0.603   \n",
       "2          0.619136        0.794339       0.764686         0.650075   \n",
       "3          0.654318        0.664056       0.791714         0.648363   \n",
       "4          0.618715        0.746867       0.758392         0.577344   \n",
       "...             ...             ...            ...              ...   \n",
       "22596      0.621547        0.670748       0.774785         0.634767   \n",
       "22597      0.659439        0.904112       0.777663         0.685988   \n",
       "22598      0.743001        0.752128        0.81698         0.669132   \n",
       "22599      0.618995        0.703332       0.785767         0.622811   \n",
       "22600      0.643032        0.913813       0.759902         0.644197   \n",
       "\n",
       "      guard_racism guard_eatingDisorders  \n",
       "0         0.623649              0.817572  \n",
       "1         0.538758              0.753499  \n",
       "2         0.465704              0.784149  \n",
       "3         0.640479              0.819716  \n",
       "4         0.443122               0.75944  \n",
       "...            ...                   ...  \n",
       "22596     0.519427              0.774898  \n",
       "22597     0.485242                 0.791  \n",
       "22598     0.658693              0.829151  \n",
       "22599     0.562866              0.796512  \n",
       "22600     0.465164              0.765236  \n",
       "\n",
       "[22592 rows x 6 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles_embeddings = calculate_trigger_scores_df(articles,trigger_vecs)\n",
    "\n",
    "# Rename columns\n",
    "articles_embeddings = articles_embeddings.rename(columns={'zelfmoord': 'guard_suicide', 'ongelukken': 'guard_accidents','zelfbeschadiging': 'guard_selfHarm','depressie':'guard_depression',\n",
    "                        'racisme':'guard_racism','eetstoornissen':'guard_eatingDisorders'})\n",
    "\n",
    "# Save dataframe as CSV file\n",
    "articles_embeddings.to_csv('articles_embeddings.csv', index=False)  \n",
    "\n",
    "articles_embeddings  \n",
    "\n",
    "#Limitation: If a word is not present in the dictionary (in wordEmbeddings.wv.key_to_index), \n",
    "#  the findSimilarWords() function will not add it to the similarWordsDict dictionary.\n",
    "#  And that trigger word will not have any associated similar words in the output. For this reason, only 6 of the triggers are in the final dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
